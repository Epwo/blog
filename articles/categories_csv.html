<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><link rel="preload" href="https://raw.githubusercontent.com/Epwo/articles/refs/heads/main/images/nlp_exp/nlp_exp_header.jpg" as="image" data-next-head=""/><link rel="preload" href="/blog/_next/static/media/4cf2300e9c8272f7-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/blog/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/blog/_next/static/css/1f148c948bb4c78d.css" as="style"/><link rel="preload" href="/blog/_next/static/css/1cb85b1c355f1fa8.css" as="style"/><link rel="stylesheet" href="/blog/_next/static/css/1f148c948bb4c78d.css" data-n-g=""/><link rel="stylesheet" href="/blog/_next/static/css/1cb85b1c355f1fa8.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/blog/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/blog/_next/static/chunks/webpack-8b8747d91b84f23b.js" defer=""></script><script src="/blog/_next/static/chunks/framework-2f335d22a7318891.js" defer=""></script><script src="/blog/_next/static/chunks/main-dad8c2de6c2b05ba.js" defer=""></script><script src="/blog/_next/static/chunks/pages/_app-5490ad400bc1e6e8.js" defer=""></script><script src="/blog/_next/static/chunks/996-169541a85194d51f.js" defer=""></script><script src="/blog/_next/static/chunks/476-39bbd294c5186124.js" defer=""></script><script src="/blog/_next/static/chunks/pages/articles/%5Bslug%5D-d0a68914715baca5.js" defer=""></script><script src="/blog/_next/static/sDZWojPIZvHo2uz6glSJH/_buildManifest.js" defer=""></script><script src="/blog/_next/static/sDZWojPIZvHo2uz6glSJH/_ssgManifest.js" defer=""></script></head><body><link rel="preload" as="image" href="https://raw.githubusercontent.com/Epwo/articles/refs/heads/main/images/nlp_exp/nlp_exp_header.jpg"/><link rel="preload" as="image" href="https://raw.githubusercontent.com/Epwo/articles/refs/heads/main/images/nlp_exp/image.png"/><link rel="preload" as="image" href="https://raw.githubusercontent.com/Epwo/articles/refs/heads/main/images/nlp_exp/step1.png"/><link rel="preload" as="image" href="https://github.com/Epwo/articles/blob/main/images/nlp_exp/purify_data.png?raw=true"/><link rel="preload" as="image" href="https://miro.medium.com/v2/resize:fit:1400/1*fGQImmija7kepddB7SFaGA.jpeg"/><link rel="preload" as="image" href="https://github.com/Epwo/articles/blob/main/images/nlp_exp/outliers.png?raw=true"/><link rel="preload" as="image" href="https://github.com/Epwo/articles/blob/main/images/nlp_exp/dbscan_general.png?raw=true"/><link rel="preload" as="image" href="https://github.com/Epwo/articles/blob/main/images/nlp_exp/dbscan_detailled.png?raw=true"/><link rel="preload" as="image" href="https://github.com/Epwo/articles/blob/main/images/nlp_exp/reglin.png?raw=true"/><link rel="preload" as="image" href="https://github.com/Epwo/articles/blob/main/images/nlp_exp/RF.png?raw=true"/><link rel="preload" as="image" href="https://github.com/Epwo/articles/blob/main/images/nlp_exp/visualize.png?raw=true"/><link rel="preload" as="image" href="https://github.com/Epwo/articles/blob/main/images/nlp_exp/centrals_elements.png?raw=true"/><link rel="preload" as="image" href="https://github.com/Epwo/articles/blob/main/images/nlp_exp/inanutshell.png?raw=true"/><div id="__next"><main class="__variable_7ddc9d __variable_130274"><div class="HeaderImage_headerImageContainer__Khgdd"><div class="HeaderImage_imageWrapper__naSNh"><img alt="Header image for How to extract categories from a csv file" decoding="async" data-nimg="fill" class="HeaderImage_headerImage__T4YXT" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="https://raw.githubusercontent.com/Epwo/articles/refs/heads/main/images/nlp_exp/nlp_exp_header.jpg"/><div class="HeaderImage_imageFadeOverlay__B1Dny"></div></div></div><div class="ArticleNavBar_navBar__XXLij"><div class="ArticleNavBar_navBarContent__OnqKO"><a class="ArticleNavBar_backButton__DnCpY" href="/blog"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-left" aria-hidden="true"><path d="m12 19-7-7 7-7"></path><path d="M19 12H5"></path></svg></a><div class="ArticleNavBar_breadcrumbs__3EEIn"><a class="ArticleNavBar_breadcrumbItem__leJnr" href="/blog">Home</a><span class="ArticleNavBar_breadcrumbSeparator__NYFW8">/</span><span class="ArticleNavBar_breadcrumbCurrent__Syolk">How to extract categories from a csv file</span></div></div></div><article class="page_articleDetail___FGo1"><div class="page_articleHeader__ZEI9R"><h1>How to extract categories from a csv file</h1><div class="page_articleMeta__eG2iP"><p class="page_date__EZD2O">in June</p><span class="ThemeTag_themeTag__xXwtj"><span class="ThemeTag_themeTagContent__NqEtM"><span class="ThemeTag_emoji__a5dW3">ðŸ’»</span>Coding</span><span class="ThemeTag_themeTagContent__NqEtM"><span class="ThemeTag_emoji__a5dW3">ðŸ§ </span>ML</span></span></div><p class="page_summary__g9kyb">Explanation of my pipeline including NLP, and what I&#x27;ve learned</p></div><h1>The purpose</h1>
<p>The purpose of this task, was to find and extracts categories from a csv containing feedback (verbtatims);</p>
<h2>example</h2>
<p>to illustrate, if we take the disneyland review csv, we want to extract like; <code>parking problems</code> , <code>waiting line issues</code>, <code>safety</code> etc..</p>
<h1>The project ( topic-eur )</h1>
<p>That is how the project was born, name is topic-eur because it is a topic crawler -&gt; topic-eur (which also is a pokemon!)</p>
<p>I did not really have a plan or hints that were given to me, so it was a fun research data science project.</p>
<h1>The idea</h1>
<p>My idea was as follow:
To get the meaning of each verbatim, I choose to use the transformers-based models.
The main supposition I made was that, if we look at the highest values attentions score of the sentence we could extract its meaning.
This is an idea based of the human way of learning a new language, if I try to learn german. I am really bad at german, therefore I can&#x27;t understand nor translate word by word, I will however be able to understand the overall meaning of the sentence. That is because I can pick up 2 or 3 words around the subject and an adjective.
<img src="https://raw.githubusercontent.com/Epwo/articles/refs/heads/main/images/nlp_exp/image.png" alt="alt text"/></p>
<p>Therefore, if we look at the attention layer of a transformer-based model this should give us an idea of the words contaning most of the meaning of each verbatim.</p>
<h1>The model</h1>
<p>Because our verbatims will be in french, I choose to go with the latest camem-BERT model ( made by INRIA )
<a href="https://huggingface.co/almanach/moderncamembert-base">moderncamembert</a></p>
<p>So we will for each row of the given csv ( each row representing a verbatim )
<img src="https://raw.githubusercontent.com/Epwo/articles/refs/heads/main/images/nlp_exp/step1.png" alt="step one of the process"/></p>
<h1>the Attention Layer</h1>
<p>However, if you know the transformers-based models quite well. You know that there are two mains issues with just going with the data as it is.</p>
<h2>1st issue</h2>
<p>the first issue is that thoses models are trained to work on long sentences and even large texts. So they dont stop at just the &quot;.&quot; at the end.
To do that, they put a lot of attention on specials tokens that the tokenizer gives them to notify that this is the end of sentence for example.
SO we will remove thoses. we alos will remove the words that could be meaningful but in a group, not alone. like the stopwords or the punctuations.</p>
<h2>2nd Issue</h2>
<p>We also have another problem with the tokenizer, his main purpose is to divide the text into &#x27;tokens&#x27; that are words or part of a word. This last part, is what we want to get rid of. Let&#x27;s take the word &quot;magnifique&quot; as an example &gt; if &quot;magnifique&quot; becomes &quot;magn-&quot;, &quot;ifi-&quot;,&quot;que&quot; . The token that contains the most meaning might be only one of them. However it is the word that contains the most meaning not only the &quot;ifi-&quot; ( which is btw, non usable as it is).
Luckly for us, the tokenizer when splitting words doesnt just cut them in half, he also add a little &quot;thing&quot; to say that this word was cut by the tokenizer.
Which means that we can recreate thoses words !
(For camembert it is actuallly splitting the words like this : lamentable -&gt;  lam, ##entab, ##le)
So we will recreate the words that have been split, and add their attention score.</p>
<p><img src="https://github.com/Epwo/articles/blob/main/images/nlp_exp/purify_data.png?raw=true" alt="the steps to &quot;purify&quot; the data"/></p>
<h1>Data science</h1>
<p>Now we have as the output of the first step, a dict of each verbatim, where each one has the 5 most &quot;meaningful&quot; words ( the tensor value &amp; the word itself )
The idea is that, the model (here the BERT-based model) is determinist. He will (roughly) give the same emplacement in the latent space for two words that have the same meaning.</p>
<p>However, BERT is giving a 758 dimensions tensor as an input. The issue is that we as humans, cant represent this many dimensions. And that the usal data sciences algortihms are not fit for this many dimensions. ( or at least what I will be using.)
this is where umaps comes into play</p>
<h2>UMAP</h2>
<p>UMAP is pretty robsut algorithm used to reduce the dimensions :
<img src="https://miro.medium.com/v2/resize:fit:1400/1*fGQImmija7kepddB7SFaGA.jpeg" alt="umap_image"/></p>
<p>it is known to be quite good at loosing less informations in the process than the other methods ( at least from what I&#x27;ve read. )
I choose to go with UMAP, for the dimensions reduction. We are going from 759 to 3, to be able to make a 3D graph representation of the words.</p>
<h2>Outliers</h2>
<p>As often when dealing with big data, it appears to have what we call outliers.
Thoses are values that are not pertinent, and can often disturb the clustering algorithms.
<img src="https://github.com/Epwo/articles/blob/main/images/nlp_exp/outliers.png?raw=true" alt="example of an outlier"/></p>
<p>I have choose to go with KNN to remove the outliers.</p>
<h2>The (real) clustering</h2>
<p>Durings my research to find the best algorithm to clusterise my type of data. I&#x27;ve stumble upon DBSCAN, which ,I think is, quite awesome
<img src="https://github.com/Epwo/articles/blob/main/images/nlp_exp/dbscan_general.png?raw=true" alt="dbscan"/></p>
<p>DBSCAN, base itself on two values defined by the user, the &#x27;eps&#x27; value and the &#x27;min_samples&#x27; value.
If we take a first point A, the &#x27;eps&#x27; will be the size of the range to allow another point B to be classifed in the same cluster (A+B) (see the schematic I drew below)
The &#x27;min_values&#x27; is simply the number of points a group needs to have (minimum) to be consider a real cluster.
<img src="https://github.com/Epwo/articles/blob/main/images/nlp_exp/dbscan_detailled.png?raw=true" alt="how it works"/></p>
<p>The awesome thing with this method, is that the user virtually dosent have to choose the number of clusters. (because in my case, this number is quite changing, depending on the csv we are analyzing)
However, while we don&#x27;t have to choose a number of clusters. We still need to define the value of both &#x27;eps&#x27; and &#x27;min_values&#x27;.</p>
<h3>Choosing the &#x27;range&#x27;(eps) and &#x27;min_values&#x27;</h3>
<p>I&#x27;ve tried to choose a fixed values for both. But while testing with other datasets (other sets of verbatims), I saw that it was differing too much, from one dataset to another.</p>
<p>Then I thought, that thoses values would be a simple linear regression. (like this :)
<img src="https://github.com/Epwo/articles/blob/main/images/nlp_exp/reglin.png?raw=true" alt="alt text"/></p>
<p>Turns out, it was not a linear regression either.
So i thought, what could i have and what could i use ?
I want to guess the &#x27;range&#x27; (eps) and the &#x27;min_values&#x27;. E,N
What values ( that changes, with each dataset) am i left with ? : the sigma value (standard deviation) between the points, and the total number of points (the numbers of words)</p>
<p>Therefore I was left with something like this :
f(nb_pts,sigma) = eps
g(nb_pts,sigma) = min_values</p>
<p>So to compute this f and g function, I choose to train a Random Forest model ( a Machine learning solution, amongst many)
The hard part was, on what to train it. I choose to fine-tune at hand, the eps and min_values for about ten datasets
(I know that this is not significant, but it was slow and painful. and I tried to make them as representative as possible, like at each extremums)</p>
<p>NB : Now that I am writing this, I could have gone with a Reinforcement learning, where you dont need to input any &quot;train data&quot;.</p>
<p>Once the two models were trained, I could print out some pretty graphs that looks like this
<img src="https://github.com/Epwo/articles/blob/main/images/nlp_exp/RF.png?raw=true" alt="alt text"/></p>
<h1>A representation, of the data</h1>
<p>At this point the hardest is behind us, it will be more straight forwards starting from here.
But first, to vizualise better what I was doing up to this point, and to see if the clustering method with my predicted eps ans min_values are looking good.
<img src="https://github.com/Epwo/articles/blob/main/images/nlp_exp/visualize.png?raw=true" alt="alt text"/></p>
<blockquote>
<p>[!Info]
Because there are too many clusters compared to the number of colors availbles in plotly, some clusters might look like they are from the same cluster, despite beeing in 2 different ones, but the color is the same, because the color is chosen randomly among a &quot;short&quot; list.</p>
</blockquote>
<h2>Extracting a category for each cluster</h2>
<p>Because the words close to each others are close in their meaning, each cluster (group of close words) should tend toward a category
But how to display it ? The idea is to pass elements of the cluster into a LLM and ask him to find the category that could represent thoses words, if there is one if not return &#x27;non-relevent&#x27;.
The issue is that for somes csv (most of it) there will be a lot of words in each clusters, so it would be too long to pass to the LLM all of the words for each cluster.
So I&#x27;ve chosen to take what I&#x27;ve called the centrals elements, meaning the points that are the closest to the core of the cluster;
Such as displayed on this schematic :
<img src="https://github.com/Epwo/articles/blob/main/images/nlp_exp/centrals_elements.png?raw=true" alt="central elements explanation"/></p>
<h1>To sum it up</h1>
<p>To conclude the final pipeline as a whole
<img src="https://github.com/Epwo/articles/blob/main/images/nlp_exp/inanutshell.png?raw=true" alt="central elements explanation"/></p>
<h1>The results</h1>
<p>In the end, I had to my opinion pretty good results. Which I sadly cannot share here because it was made for and during company time.
It turns out that sadly, it could not be implemented in the app we were making because the category that it would output (while beeing relevents) where not corresponding to the naming that the end-users ,in the company, used. So we end up going with a much more high-level solution (with the release of gemini 2.5 and its 1M token context window) were we litterally input all of the CSV into gemini, and ask him to categorize.
(Which is sadly a faster and more fitting solution for our precise use-case).</p></article></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"article":{"title":"How to extract categories from a csv file","date":"2025-06-30","content":"\n# The purpose\nThe purpose of this task, was to find and extracts categories from a csv containing feedback (verbtatims);\n## example\nto illustrate, if we take the disneyland review csv, we want to extract like; `parking problems` , `waiting line issues`, `safety` etc..\n\n\n# The project ( topic-eur )\nThat is how the project was born, name is topic-eur because it is a topic crawler -\u003e topic-eur (which also is a pokemon!)\n\nI did not really have a plan or hints that were given to me, so it was a fun research data science project.\n\n# The idea\nMy idea was as follow:\nTo get the meaning of each verbatim, I choose to use the transformers-based models.\nThe main supposition I made was that, if we look at the highest values attentions score of the sentence we could extract its meaning.\nThis is an idea based of the human way of learning a new language, if I try to learn german. I am really bad at german, therefore I can't understand nor translate word by word, I will however be able to understand the overall meaning of the sentence. That is because I can pick up 2 or 3 words around the subject and an adjective.\n![alt text](https://raw.githubusercontent.com/Epwo/articles/refs/heads/main/images/nlp_exp/image.png)\n\nTherefore, if we look at the attention layer of a transformer-based model this should give us an idea of the words contaning most of the meaning of each verbatim.\n# The model \nBecause our verbatims will be in french, I choose to go with the latest camem-BERT model ( made by INRIA )\n[moderncamembert](https://huggingface.co/almanach/moderncamembert-base)\n\nSo we will for each row of the given csv ( each row representing a verbatim )\n![step one of the process](https://raw.githubusercontent.com/Epwo/articles/refs/heads/main/images/nlp_exp/step1.png)\n\n# the Attention Layer\nHowever, if you know the transformers-based models quite well. You know that there are two mains issues with just going with the data as it is.\n## 1st issue\n the first issue is that thoses models are trained to work on long sentences and even large texts. So they dont stop at just the \".\" at the end.\n To do that, they put a lot of attention on specials tokens that the tokenizer gives them to notify that this is the end of sentence for example.\n SO we will remove thoses. we alos will remove the words that could be meaningful but in a group, not alone. like the stopwords or the punctuations.\n## 2nd Issue\nWe also have another problem with the tokenizer, his main purpose is to divide the text into 'tokens' that are words or part of a word. This last part, is what we want to get rid of. Let's take the word \"magnifique\" as an example \u003e if \"magnifique\" becomes \"magn-\", \"ifi-\",\"que\" . The token that contains the most meaning might be only one of them. However it is the word that contains the most meaning not only the \"ifi-\" ( which is btw, non usable as it is).\nLuckly for us, the tokenizer when splitting words doesnt just cut them in half, he also add a little \"thing\" to say that this word was cut by the tokenizer.\nWhich means that we can recreate thoses words !\n(For camembert it is actuallly splitting the words like this : lamentable -\u003e  lam, ##entab, ##le)\nSo we will recreate the words that have been split, and add their attention score.\n\n![the steps to \"purify\" the data](https://github.com/Epwo/articles/blob/main/images/nlp_exp/purify_data.png?raw=true)\n\n# Data science\nNow we have as the output of the first step, a dict of each verbatim, where each one has the 5 most \"meaningful\" words ( the tensor value \u0026 the word itself )\nThe idea is that, the model (here the BERT-based model) is determinist. He will (roughly) give the same emplacement in the latent space for two words that have the same meaning.\n\nHowever, BERT is giving a 758 dimensions tensor as an input. The issue is that we as humans, cant represent this many dimensions. And that the usal data sciences algortihms are not fit for this many dimensions. ( or at least what I will be using.)\nthis is where umaps comes into play\n## UMAP\nUMAP is pretty robsut algorithm used to reduce the dimensions :\n![umap_image](https://miro.medium.com/v2/resize:fit:1400/1*fGQImmija7kepddB7SFaGA.jpeg)\n\nit is known to be quite good at loosing less informations in the process than the other methods ( at least from what I've read. )\nI choose to go with UMAP, for the dimensions reduction. We are going from 759 to 3, to be able to make a 3D graph representation of the words.\n\n## Outliers\nAs often when dealing with big data, it appears to have what we call outliers.\nThoses are values that are not pertinent, and can often disturb the clustering algorithms. \n![example of an outlier](https://github.com/Epwo/articles/blob/main/images/nlp_exp/outliers.png?raw=true)\n\nI have choose to go with KNN to remove the outliers.\n\n## The (real) clustering\nDurings my research to find the best algorithm to clusterise my type of data. I've stumble upon DBSCAN, which ,I think is, quite awesome\n![dbscan](https://github.com/Epwo/articles/blob/main/images/nlp_exp/dbscan_general.png?raw=true)\n\nDBSCAN, base itself on two values defined by the user, the 'eps' value and the 'min_samples' value.\nIf we take a first point A, the 'eps' will be the size of the range to allow another point B to be classifed in the same cluster (A+B) (see the schematic I drew below)\nThe 'min_values' is simply the number of points a group needs to have (minimum) to be consider a real cluster.\n![how it works](https://github.com/Epwo/articles/blob/main/images/nlp_exp/dbscan_detailled.png?raw=true)\n\nThe awesome thing with this method, is that the user virtually dosent have to choose the number of clusters. (because in my case, this number is quite changing, depending on the csv we are analyzing)\nHowever, while we don't have to choose a number of clusters. We still need to define the value of both 'eps' and 'min_values'. \n\n### Choosing the 'range'(eps) and 'min_values'\nI've tried to choose a fixed values for both. But while testing with other datasets (other sets of verbatims), I saw that it was differing too much, from one dataset to another.\n\nThen I thought, that thoses values would be a simple linear regression. (like this :)\n![alt text](https://github.com/Epwo/articles/blob/main/images/nlp_exp/reglin.png?raw=true)\n\nTurns out, it was not a linear regression either.\nSo i thought, what could i have and what could i use ?\nI want to guess the 'range' (eps) and the 'min_values'. E,N\nWhat values ( that changes, with each dataset) am i left with ? : the sigma value (standard deviation) between the points, and the total number of points (the numbers of words)\n\nTherefore I was left with something like this : \nf(nb_pts,sigma) = eps \ng(nb_pts,sigma) = min_values\n\nSo to compute this f and g function, I choose to train a Random Forest model ( a Machine learning solution, amongst many)\nThe hard part was, on what to train it. I choose to fine-tune at hand, the eps and min_values for about ten datasets \n(I know that this is not significant, but it was slow and painful. and I tried to make them as representative as possible, like at each extremums)\n\nNB : Now that I am writing this, I could have gone with a Reinforcement learning, where you dont need to input any \"train data\".\n\nOnce the two models were trained, I could print out some pretty graphs that looks like this\n![alt text](https://github.com/Epwo/articles/blob/main/images/nlp_exp/RF.png?raw=true)\n\n\n# A representation, of the data\nAt this point the hardest is behind us, it will be more straight forwards starting from here.\nBut first, to vizualise better what I was doing up to this point, and to see if the clustering method with my predicted eps ans min_values are looking good.\n![alt text](https://github.com/Epwo/articles/blob/main/images/nlp_exp/visualize.png?raw=true)\n\n\u003e [!Info]\n\u003e Because there are too many clusters compared to the number of colors availbles in plotly, some clusters might look like they are from the same cluster, despite beeing in 2 different ones, but the color is the same, because the color is chosen randomly among a \"short\" list.\n\n## Extracting a category for each cluster\n\nBecause the words close to each others are close in their meaning, each cluster (group of close words) should tend toward a category\nBut how to display it ? The idea is to pass elements of the cluster into a LLM and ask him to find the category that could represent thoses words, if there is one if not return 'non-relevent'.\nThe issue is that for somes csv (most of it) there will be a lot of words in each clusters, so it would be too long to pass to the LLM all of the words for each cluster.\nSo I've chosen to take what I've called the centrals elements, meaning the points that are the closest to the core of the cluster; \nSuch as displayed on this schematic :\n![central elements explanation](https://github.com/Epwo/articles/blob/main/images/nlp_exp/centrals_elements.png?raw=true)\n\n# To sum it up\nTo conclude the final pipeline as a whole\n![central elements explanation](https://github.com/Epwo/articles/blob/main/images/nlp_exp/inanutshell.png?raw=true)\n\n\n# The results\nIn the end, I had to my opinion pretty good results. Which I sadly cannot share here because it was made for and during company time.\nIt turns out that sadly, it could not be implemented in the app we were making because the category that it would output (while beeing relevents) where not corresponding to the naming that the end-users ,in the company, used. So we end up going with a much more high-level solution (with the release of gemini 2.5 and its 1M token context window) were we litterally input all of the CSV into gemini, and ask him to categorize.\n(Which is sadly a faster and more fitting solution for our precise use-case).","image":"https://raw.githubusercontent.com/Epwo/articles/refs/heads/main/images/nlp_exp/nlp_exp_header.jpg","theme":"Coding,ML","summary":"Explanation of my pipeline including NLP, and what I've learned"}},"__N_SSG":true},"page":"/articles/[slug]","query":{"slug":"categories_csv"},"buildId":"sDZWojPIZvHo2uz6glSJH","assetPrefix":"/blog","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>