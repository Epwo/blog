{"pageProps":{"article":{"title":"potichat (nanochat) - lets remake chatgpt ?!","date":"2025-16-10","content":"\nInspired by karapathy's awesome blog post about nanochat, and then followed by the ppl from hugging face.\nI will try to train an LLM from scratch, to better understand the ins and outs.\nI will be following HF's free course at first [you can find it here](https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook#super-power-speed-and-data)\n\n# Starting\n\n## The architecture type.\n\nI've chosen to take the same baseline as google's Gemma or Alibaba's Qwen or even HF's SmolLM.\nThe architecture baseline will be _Dense_. I've Chosen to go with this one rather than an MOE or hybrid because of the final size and parameters of the model.\nI still would like to avoid training a 400B model, because it will take too much time, and therefore would be too costy for an experiment (and because I'm impatient :p)\nSo Let's aim for a >10B model.\n\n## The training framework\n\nIn the same spirit as before, I've chosen to go with **TorchTitan** ([here]([https://github.com/pytorch/torchtitan)) because while It was only tested by the pytorch team and is relatively new, It is optimized for Dense type.\nIt is also much lighter, and therefore (I hope) will take me less time to go through if needed.\n\n_WIP_\n","image":"https://ph-files.imgix.net/36810de6-302c-443a-90f3-763a9757fc01.png?auto=format&fit=crop","theme":"Coding,ML","summary":"making a chatgpt like from scratch to naviguate the whole stack"}},"__N_SSG":true}